{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, predict_index):\n",
    "    data = pd.read_csv(filename)\n",
    "    \n",
    "    data_in_1 = data[data[predict_index] == 1]\n",
    "    data_in_0 = data[data[predict_index] == 0]\n",
    "    \n",
    "    data_in_1_arr2d = data_in_1.values\n",
    "    data_in_0_arr2d = data_in_0.values\n",
    "    \n",
    "    np.random.shuffle(data_in_1_arr2d)\n",
    "    np.random.shuffle(data_in_0_arr2d)\n",
    "    \n",
    "    train_data_in_1_arr2d = data_in_1_arr2d[:3500 ,:]\n",
    "    train_data_in_0_arr2d = data_in_0_arr2d[:5500 ,:]\n",
    "    train_data_arr2d = np.concatenate((train_data_in_1_arr2d, train_data_in_0_arr2d), axis=0)\n",
    "    np.random.shuffle(train_data_arr2d)\n",
    "    \n",
    "    #print(train_data_in_1_arr2d.shape, train_data_in_0_arr2d.shape, train_data_arr2d.shape)\n",
    "    \n",
    "    X = train_data_arr2d[:, :train_data_arr2d.shape[1] - 1]\n",
    "    y = train_data_arr2d[:, train_data_arr2d.shape[1] - 1].T\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "    \n",
    "    sc = MinMaxScaler()\n",
    "    train_x = sc.fit_transform(train_x)\n",
    "    test_x = sc.fit_transform(test_x)\n",
    "    \n",
    "    '''\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(train_x)\n",
    "    train_x = sc.transform(train_x)\n",
    "    test_x = sc.transform(test_x)\n",
    "    '''\n",
    "    return train_x.T, test_x.T, train_y.reshape(1, train_y.shape[0]), test_y.reshape(1, test_y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('datasets/train_signs.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    " \n",
    "    test_dataset = h5py.File('datasets/test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    " \n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "train_x, test_x, train_y, test_y = load_data(\"card.csv\", 'Buy')\n",
    "print(train_x, '\\n', train_x.shape, '\\n', train_y, '\\n', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    X = tf.placeholder(tf.float32, shape=[n_x, None])\n",
    "    Y = tf.placeholder(tf.float32, shape=[n_y, None])\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    #parameters['W1'] = tf.get_variable(name='W1', [layer_dims[1], layer_dims[1-1]], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W'+str(l)] = tf.get_variable('W'+str(l), [layer_dims[l], layer_dims[l-1]], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "        parameters['b'+str(l)] = tf.get_variable('b'+str(l), [layer_dims[l], 1], initializer=tf.zeros_initializer())\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "#test\n",
    "parameters = initialize_parameters_deep((5, 4, 3))\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    L = len(parameters) // 2\n",
    "    A = list(range(L+1))\n",
    "    Z = list(range(L+1))\n",
    "    \n",
    "    A[0] = X\n",
    "    for l in range(1, L):\n",
    "        Z[l] = tf.add(tf.matmul(parameters['W'+str(l)], A[l-1]), parameters['b'+str(l)])\n",
    "        A[l] = tf.nn.relu(Z[l])\n",
    "    \n",
    "    Z[L] = tf.add(tf.matmul(parameters['W'+str(L)], A[L-1]), parameters['b'+str(L)])\n",
    "    \n",
    "    return Z[L]\n",
    "\n",
    "#test\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(26, 1)\n",
    "    parameters = initialize_parameters_deep([26, 20, 14, 8, 2, 1])\n",
    "    Z2 = forward_propagation(X, parameters)\n",
    "    print(\"Z2 = \" + str(Z2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(ZL, Y):\n",
    "    logits = tf.transpose(ZL)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    return cost\n",
    "\n",
    "#test\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(26, 1)\n",
    "    parameters = initialize_parameters_deep([26, 20, 14, 8, 2, 1])\n",
    "    ZL = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(ZL, Y)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    " \n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model(train_x, train_y, test_x, test_y, learning_rate=0.0001, epochs=1500, minibatch_size=32, print_cost=True):\n",
    "    ops.reset_default_graph()\n",
    "    tf.set_random_seed(1)\n",
    "    seed = 3\n",
    "    \n",
    "    n_x = train_x.shape[0]\n",
    "    n_y = train_y.shape[0]\n",
    "    m = train_x.shape[1]\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    parameters = initialize_parameters_deep((26, 20, 14, 8, 1))\n",
    "    \n",
    "    ZL = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(ZL, Y)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(epochs):\n",
    "            epoch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size)\n",
    "            seed += 1\n",
    "            minibatches = random_mini_batches(train_x, train_y, minibatch_size, seed)\n",
    "            \n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                #print(minibatch_cost)\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "                \n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "        \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        \n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(ZL), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: train_x, Y: train_y}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: test_x, Y: test_y}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = L_model(train_x, train_y, test_x, test_y, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
